import { Injectable } from '@nestjs/common';
import type { Responses } from 'openai/resources/responses';
import { LoggerService } from '../../../common/services/logger.service';
import {
  StreamState,
  SSEEvent,
} from '../../interfaces/streaming-events.interface';

/**
 * Handler service for lifecycle streaming events in the orchestrator pattern
 *
 * Processes 7 core lifecycle events that track the response generation lifecycle from
 * initialization through completion or failure. These events provide real-time status
 * updates about the response being generated by OpenAI's Responses API.
 *
 * **Events Handled:**
 * - `response.created` - Response initialization with ID and model
 * - `response.queued` - Response queued for processing
 * - `response.in_progress` - Generation has started
 * - `response.completed` - Generation finished successfully with usage stats
 * - `response.incomplete` - Generation stopped before completion
 * - `response.failed` - Generation failed with error details
 * - `error` - Generic error event
 *
 * Part of the handler delegation pattern where the main OpenAIResponsesService
 * orchestrates streaming by routing events to specialized handlers for processing.
 *
 * @see {@link https://platform.openai.com/docs/api-reference/responses-streaming}
 */
@Injectable()
export class LifecycleEventsHandler {
  constructor(private readonly loggerService: LoggerService) {}

  /**
   * Handle response.created event - Response initialization
   *
   * Emitted when a response is first created. Captures the response ID and model
   * for tracking throughout the stream. This is always the first event received.
   *
   * @param event - Raw event data containing response.id and response.model
   * @param state - Shared streaming state for accumulating data
   * @param sequence - Event sequence number for ordering
   * @returns Generator yielding SSE event with response_id and model
   * @yields SSEEvent with event='response_created' and response metadata
   */
  *handleResponseCreated(
    event: unknown,
    state: StreamState,
    sequence: number,
  ): Iterable<SSEEvent> {
    const eventData =
      (event as { response?: { id?: string; model?: string } }) || {};
    if (eventData.response?.id) {
      state.responseId = eventData.response.id;
      state.model = eventData.response.model;
    }

    this.loggerService.logStreamingEvent({
      timestamp: new Date().toISOString(),
      api: 'responses',
      endpoint: '/v1/responses (stream)',
      event_type: 'response_created',
      sequence,
      response: eventData.response,
    });

    yield {
      event: 'response_created',
      data: JSON.stringify({
        response_id: state.responseId,
        model: state.model,
        sequence,
      }),
      sequence,
    };
  }

  /**
   * Handle response.completed event - Final response with usage stats
   *
   * Emitted when generation completes successfully. Contains the final Response object
   * with complete usage statistics including tokens, cost estimates, and metadata.
   * This is typically the last event in a successful stream.
   *
   * @param event - Raw event data containing complete Responses.Response object
   * @param state - Shared streaming state with accumulated output text
   * @param sequence - Event sequence number for ordering
   * @param extractUsage - Utility function to extract token usage from response
   * @param extractResponseMetadata - Utility function to extract metadata (status, conversation, etc.)
   * @param estimateCost - Utility function to calculate cost from token usage
   * @returns Generator yielding SSE event with full response data and usage stats
   * @yields SSEEvent with event='response_completed', output_text, usage, and latency_ms
   */
  *handleResponseCompleted(
    event: unknown,
    state: StreamState,
    sequence: number,
    extractUsage: (response: Responses.Response) => {
      prompt_tokens?: number;
      completion_tokens?: number;
      total_tokens?: number;
      cached_tokens?: number;
      reasoning_tokens?: number;
    } | null,
    extractResponseMetadata: (response: Responses.Response) => {
      status?: string;
      error?: unknown;
      incomplete_details?: unknown;
      conversation?: unknown;
      background?: boolean | null;
      max_output_tokens?: number | null;
      previous_response_id?: string | null;
    },
    estimateCost: (
      usage: {
        prompt_tokens?: number;
        completion_tokens?: number;
        total_tokens?: number;
      } | null,
      model?: string,
    ) => number,
  ): Iterable<SSEEvent> {
    const eventData = (event as { response?: Responses.Response }) || {};
    const response = eventData.response;

    if (response) {
      state.finalResponse = response;
      const usage = extractUsage(response);
      const responseMetadata = extractResponseMetadata(response);
      const latency = Date.now() - state.startTime;

      this.loggerService.logStreamingEvent({
        timestamp: new Date().toISOString(),
        api: 'responses',
        endpoint: '/v1/responses (stream)',
        event_type: 'response_completed',
        sequence,
        response,
        metadata: {
          latency_ms: latency,
          tokens_used: usage?.total_tokens,
          cost_estimate: estimateCost(usage, response.model),
        },
      });

      yield {
        event: 'response_completed',
        data: JSON.stringify({
          response_id: state.responseId,
          output_text: state.fullText,
          usage,
          status: responseMetadata.status,
          latency_ms: latency,
          sequence,
        }),
        sequence,
      };
    }
  }

  /**
   * Handle response.failed event - Response generation failed
   *
   * Emitted when response generation fails due to errors. Contains error details
   * and partial response data if available. Clients should handle this gracefully.
   *
   * @param event - Raw event data with error object and partial response
   * @param state - Shared streaming state (may contain partial output)
   * @param sequence - Event sequence number for ordering
   * @returns Generator yielding SSE event with error details
   * @yields SSEEvent with event='response_failed' and error information
   */
  *handleResponseFailed(
    event: unknown,
    state: StreamState,
    sequence: number,
  ): Iterable<SSEEvent> {
    const eventData = (event as { error?: unknown; response?: unknown }) || {};

    this.loggerService.logStreamingEvent({
      timestamp: new Date().toISOString(),
      api: 'responses',
      endpoint: '/v1/responses (stream)',
      event_type: 'response_failed',
      sequence,
      error: eventData.error,
      response: eventData.response,
    });

    yield {
      event: 'response_failed',
      data: JSON.stringify({
        response_id: state.responseId,
        error: eventData.error,
        sequence,
      }),
      sequence,
    };
  }

  /**
   * Handle error event - Generic error
   *
   * Emitted for generic errors during streaming that don't fit other error categories.
   * Can occur at any point in the stream for network issues, timeouts, or API errors.
   *
   * @param event - Raw event data containing error object
   * @param state - Shared streaming state
   * @param sequence - Event sequence number for ordering
   * @returns Generator yielding SSE event with error details
   * @yields SSEEvent with event='error' and error information
   */
  *handleErrorEvent(
    event: unknown,
    state: StreamState,
    sequence: number,
  ): Iterable<SSEEvent> {
    const eventData = (event as { error?: unknown }) || {};

    this.loggerService.logStreamingEvent({
      timestamp: new Date().toISOString(),
      api: 'responses',
      endpoint: '/v1/responses (stream)',
      event_type: 'error',
      sequence,
      error: eventData.error,
    });

    yield {
      event: 'error',
      data: JSON.stringify({
        error: eventData.error,
        sequence,
      }),
      sequence,
    };
  }

  /**
   * Handle response.in_progress event - Response generation started
   *
   * Emitted when the response transitions from queued to actively generating.
   * Indicates the model has started processing the request.
   *
   * @param event - Raw event data (typically minimal)
   * @param state - Shared streaming state with response_id
   * @param sequence - Event sequence number for ordering
   * @returns Generator yielding SSE event signaling generation start
   * @yields SSEEvent with event='response_in_progress' and response_id
   */
  *handleResponseInProgress(
    event: unknown,
    state: StreamState,
    sequence: number,
  ): Iterable<SSEEvent> {
    this.loggerService.logStreamingEvent({
      timestamp: new Date().toISOString(),
      api: 'responses',
      endpoint: '/v1/responses (stream)',
      event_type: 'response_in_progress',
      sequence,
    });

    yield {
      event: 'response_in_progress',
      data: JSON.stringify({ response_id: state.responseId, sequence }),
      sequence,
    };
  }

  /**
   * Handle response.incomplete event - Response stopped before completion
   *
   * Emitted when generation stops before completing, typically due to max_tokens limit,
   * length constraints, or user cancellation. Contains incomplete_details explaining why.
   *
   * @param event - Raw event data with incomplete_details (reason for stopping)
   * @param state - Shared streaming state (contains partial output)
   * @param sequence - Event sequence number for ordering
   * @returns Generator yielding SSE event with incomplete details
   * @yields SSEEvent with event='response_incomplete' and incomplete_details
   */
  *handleResponseIncomplete(
    event: unknown,
    state: StreamState,
    sequence: number,
  ): Iterable<SSEEvent> {
    const eventData = (event as { incomplete_details?: unknown }) || {};

    this.loggerService.logStreamingEvent({
      timestamp: new Date().toISOString(),
      api: 'responses',
      endpoint: '/v1/responses (stream)',
      event_type: 'response_incomplete',
      sequence,
      response: { incomplete_details: eventData.incomplete_details },
    });

    yield {
      event: 'response_incomplete',
      data: JSON.stringify({
        response_id: state.responseId,
        incomplete_details: eventData.incomplete_details,
        sequence,
      }),
      sequence,
    };
  }

  /**
   * Handle response.queued event - Response queued for processing
   *
   * Emitted when the response is queued and waiting for processing resources.
   * Common during high load periods. Follows response.created event.
   *
   * @param event - Raw event data (typically minimal)
   * @param state - Shared streaming state with response_id
   * @param sequence - Event sequence number for ordering
   * @returns Generator yielding SSE event indicating queued status
   * @yields SSEEvent with event='response_queued' and response_id
   */
  *handleResponseQueued(
    event: unknown,
    state: StreamState,
    sequence: number,
  ): Iterable<SSEEvent> {
    this.loggerService.logStreamingEvent({
      timestamp: new Date().toISOString(),
      api: 'responses',
      endpoint: '/v1/responses (stream)',
      event_type: 'response_queued',
      sequence,
    });

    yield {
      event: 'response_queued',
      data: JSON.stringify({ response_id: state.responseId, sequence }),
      sequence,
    };
  }
}
