import { Injectable, Inject } from '@nestjs/common';
import OpenAI, { toFile } from 'openai';
import { OPENAI_CLIENT } from '../providers/openai-client.provider';
import type { Files } from 'openai/resources/files';
import { LoggerService } from '../../common/services/logger.service';

/**
 * Service for interacting with OpenAI Files API
 *
 * **Purpose**: File storage and management for OpenAI operations.
 * Supports uploading, retrieving, listing, deleting, and downloading files.
 *
 * **Supported Operations**:
 * - File Upload: Store files for batch processing, fine-tuning, assistants, vision
 * - File Management: Retrieve metadata, list files, delete files
 * - File Download: Download processed batch outputs and fine-tune results
 * - Polling: Monitor file processing status with exponential backoff
 *
 * **File Purposes**:
 * - `assistants`: Files for assistant knowledge retrieval
 * - `assistants_output`: Output files generated by assistants
 * - `batch`: Input files for batch API requests (JSONL format)
 * - `batch_output`: Output files from batch processing
 * - `fine-tune`: Training data for model fine-tuning (JSONL format)
 * - `fine-tune-results`: Results from fine-tuning operations
 * - `vision`: Image files for vision model processing
 *
 * **File Size Limits**:
 * - Maximum 512 MB per file
 * - JSONL files: Recommended < 100 MB for optimal processing
 *
 * @see {@link https://platform.openai.com/docs/api-reference/files}
 */
@Injectable()
export class OpenAIFilesService {
  constructor(
    @Inject(OPENAI_CLIENT) private readonly client: OpenAI,
    private readonly loggerService: LoggerService,
  ) {}

  /**
   * Upload a file to OpenAI for processing or storage
   *
   * **Purpose Types**:
   * - `assistants`: Knowledge retrieval for assistants (PDF, TXT, DOCX, JSON, etc.)
   * - `batch`: Batch API input (JSONL format only, max 100 MB recommended)
   * - `fine-tune`: Model training data (JSONL format only)
   * - `vision`: Image analysis (PNG, JPG, WEBP, GIF)
   *
   * **File Size Limit**: 512 MB maximum
   *
   * **Expiration Policy** (`expires_after`):
   * - `anchor`: 'last_active_at' (delete after inactivity)
   * - `days`: 1-365 (retention period)
   * - Default: Files persist until manually deleted
   *
   * **Processing Status**:
   * - `uploaded`: Initial state, file received
   * - `processed`: Ready for use (may take seconds to minutes)
   * - `error`: Processing failed (check status_details)
   *
   * @param fileBuffer - File content as Buffer (max 512 MB)
   * @param filename - Original filename with extension
   * @param purpose - Intended use case for the file
   * @param expiresAfter - Optional expiration policy
   * @returns OpenAI FileObject with metadata
   *
   * @example
   * ```typescript
   * // Upload batch input file
   * const batchFile = await service.uploadFile(
   *   Buffer.from(jsonlContent),
   *   'batch-input.jsonl',
   *   'batch'
   * );
   *
   * // Upload assistant file with expiration
   * const assistantFile = await service.uploadFile(
   *   pdfBuffer,
   *   'document.pdf',
   *   'assistants',
   *   { anchor: 'last_active_at', days: 30 }
   * );
   *
   * // Upload vision file
   * const visionFile = await service.uploadFile(
   *   imageBuffer,
   *   'image.png',
   *   'vision'
   * );
   * ```
   */
  async uploadFile(
    fileBuffer: Buffer,
    filename: string,
    purpose: Files.FilePurpose,
    expiresAfter?: Files.FileCreateParams['expires_after'],
  ): Promise<Files.FileObject> {
    const startTime = Date.now();

    const params: Files.FileCreateParams = {
      file: await toFile(fileBuffer, filename),
      purpose,
      ...(expiresAfter && { expires_after: expiresAfter }),
    };

    const file: Files.FileObject = await this.client.files.create(params);

    // Log interaction (no data modification)
    this.loggerService.logOpenAIInteraction({
      timestamp: new Date().toISOString(),
      api: 'files',
      endpoint: '/v1/files',
      request: {
        filename,
        purpose,
        bytes: fileBuffer.length,
        expires_after: expiresAfter,
      },
      response: file,
      metadata: {
        latency_ms: Date.now() - startTime,
        file_id: file.id,
        filename: file.filename,
        bytes: file.bytes,
        purpose: file.purpose,
        status: file.status,
        created_at: file.created_at,
      },
    });

    return file; // Return OpenAI response as-is
  }

  /**
   * Retrieve file metadata by ID
   *
   * **Use Cases**:
   * - Check file processing status (uploaded, processed, error)
   * - Get file size and creation timestamp
   * - Verify file purpose and expiration settings
   *
   * @param fileId - File ID (starts with "file-")
   * @returns OpenAI FileObject with metadata
   *
   * @example
   * ```typescript
   * const file = await service.retrieveFile('file-abc123xyz789');
   * console.log(`Status: ${file.status}, Size: ${file.bytes} bytes`);
   * ```
   */
  async retrieveFile(fileId: string): Promise<Files.FileObject> {
    const startTime = Date.now();

    const file: Files.FileObject = await this.client.files.retrieve(fileId);

    this.loggerService.logOpenAIInteraction({
      timestamp: new Date().toISOString(),
      api: 'files',
      endpoint: `/v1/files/${fileId}`,
      request: {},
      response: file,
      metadata: {
        latency_ms: Date.now() - startTime,
        file_id: file.id,
        filename: file.filename,
        bytes: file.bytes,
        purpose: file.purpose,
        status: file.status,
      },
    });

    return file;
  }

  /**
   * List all files with optional filtering and pagination
   *
   * **Pagination**:
   * - `limit`: Number of results per page (default: 10,000)
   * - `order`: Sort by creation time ('asc' or 'desc', default: 'desc')
   *
   * **Filtering**:
   * - `purpose`: Filter by file purpose (assistants, batch, fine-tune, vision)
   * - Returns all purposes if not specified
   *
   * @param purpose - Optional filter by purpose
   * @param order - Sort order by created_at: 'asc' or 'desc'
   * @param limit - Number of files to return
   * @returns Array of OpenAI FileObject
   *
   * @example
   * ```typescript
   * // List all files (most recent first)
   * const allFiles = await service.listFiles();
   *
   * // Filter by purpose
   * const batchFiles = await service.listFiles('batch');
   *
   * // Pagination with limit
   * const recentFiles = await service.listFiles(undefined, 'desc', 50);
   * ```
   */
  async listFiles(
    purpose?: string,
    order: 'asc' | 'desc' = 'desc',
    limit?: number,
  ): Promise<Files.FileObject[]> {
    const startTime = Date.now();

    const params: Files.FileListParams = {
      ...(purpose && { purpose }),
      ...(order && { order }),
      ...(limit && { limit }),
    };

    const page = await this.client.files.list(params);

    this.loggerService.logOpenAIInteraction({
      timestamp: new Date().toISOString(),
      api: 'files',
      endpoint: '/v1/files',
      request: params,
      response: page.data,
      metadata: {
        latency_ms: Date.now() - startTime,
        result_count: page.data.length,
      },
    });

    return page.data; // Return data array as-is
  }

  /**
   * Delete file from OpenAI storage
   *
   * **Important Notes**:
   * - Cannot delete files currently in use by assistants or active batches
   * - Deletion is immediate and permanent (no undo)
   * - Files with expiration policy will auto-delete at expiry
   *
   * @param fileId - File ID to delete
   * @returns OpenAI deletion confirmation response
   *
   * @example
   * ```typescript
   * const result = await service.deleteFile('file-abc123xyz789');
   * console.log(`Deleted: ${result.deleted}`); // true
   * ```
   */
  async deleteFile(fileId: string): Promise<Files.FileDeleted> {
    const startTime = Date.now();

    const result: Files.FileDeleted = await this.client.files.delete(fileId);

    this.loggerService.logOpenAIInteraction({
      timestamp: new Date().toISOString(),
      api: 'files',
      endpoint: `/v1/files/${fileId}`,
      request: {},
      response: result,
      metadata: {
        latency_ms: Date.now() - startTime,
        file_id: fileId,
        deleted: result.deleted,
      },
    });

    return result;
  }

  /**
   * Download file content as binary stream
   *
   * **Purpose Restrictions**:
   * - `assistants`: Cannot be downloaded (knowledge retrieval only)
   * - `batch_output`: Downloadable (JSONL results)
   * - `fine-tune-results`: Downloadable (training metrics)
   * - `assistants_output`: Downloadable (generated files)
   *
   * **Binary Streaming**:
   * - Returns native Response object from OpenAI SDK
   * - Use `response.arrayBuffer()` or `response.body` for streaming
   * - Content-Type header indicates file format
   *
   * @param fileId - File ID to download
   * @returns Response object from OpenAI (binary stream)
   * @throws Error if file purpose is 'assistants' (download forbidden)
   *
   * @example
   * ```typescript
   * // Download batch output
   * const response = await service.downloadFileContent('file-batch-output-123');
   * const buffer = Buffer.from(await response.arrayBuffer());
   * fs.writeFileSync('output.jsonl', buffer);
   *
   * // Download fine-tune results
   * const response = await service.downloadFileContent('file-ft-results-456');
   * const contentType = response.headers.get('content-type');
   * ```
   */
  async downloadFileContent(fileId: string): Promise<Response> {
    const startTime = Date.now();

    const response: Response = await this.client.files.content(fileId);

    this.loggerService.logOpenAIInteraction({
      timestamp: new Date().toISOString(),
      api: 'files',
      endpoint: `/v1/files/${fileId}/content`,
      request: {},
      response: {
        content_type:
          response.headers?.get('content-type') || 'application/octet-stream',
      },
      metadata: {
        latency_ms: Date.now() - startTime,
        file_id: fileId,
      },
    });

    return response;
  }

  /**
   * Poll until file processing completes
   *
   * **Polling Strategy**:
   * - Uses exponential backoff: 5s → 10s → 15s → 20s (max interval)
   * - Default timeout: 10 minutes (600,000 ms)
   * - Polls every interval until status reaches terminal state
   *
   * **Terminal Statuses**:
   * - `processed`: File ready for use
   * - `error`: Processing failed (check status_details)
   *
   * **Use Cases**:
   * - Poll after uploading batch/fine-tune files
   * - Monitor file indexing for assistants
   * - Wait for vision file processing
   *
   * @param fileId - File ID to poll
   * @param maxWaitMs - Maximum wait time (default: 10 minutes)
   * @returns Final OpenAI FileObject (status: 'processed' or 'error')
   * @throws Error if timeout exceeded
   *
   * @example
   * ```typescript
   * // Poll with default 10-minute timeout
   * const file = await service.uploadFile(buffer, 'data.jsonl', 'batch');
   * const processed = await service.pollUntilComplete(file.id);
   *
   * // Poll with custom timeout (30 seconds)
   * const processed = await service.pollUntilComplete(file.id, 30000);
   * if (processed.status === 'error') {
   *   console.error('Processing failed:', processed.status_details);
   * }
   * ```
   */
  async pollUntilComplete(
    fileId: string,
    maxWaitMs: number = 600000,
  ): Promise<Files.FileObject> {
    const startTime = Date.now();
    let waitTime = 5000; // Start with 5 seconds

    while (Date.now() - startTime < maxWaitMs) {
      const file: Files.FileObject = await this.retrieveFile(fileId);

      // Return when processing completes or fails
      if (file.status === 'processed' || file.status === 'error') {
        this.loggerService.logOpenAIInteraction({
          timestamp: new Date().toISOString(),
          api: 'files',
          endpoint: `/v1/files/${fileId}/poll`,
          request: { max_wait_ms: maxWaitMs },
          response: file,
          metadata: {
            latency_ms: Date.now() - startTime,
            file_id: file.id,
            filename: file.filename,
            bytes: file.bytes,
            purpose: file.purpose,
            status: file.status,
          },
        });

        return file; // Return OpenAI response as-is
      }

      // Wait before next poll (exponential backoff)
      await this.sleep(waitTime);
      waitTime = Math.min(waitTime + 5000, 20000); // Cap at 20 seconds
    }

    // Timeout exceeded
    throw new Error(`File ${fileId} did not complete within ${maxWaitMs}ms`);
  }

  /**
   * Extract file metadata from OpenAI response
   *
   * **Purpose**: Helper method to structure data for readability (no computation).
   * Useful for displaying file information in UI or logs.
   *
   * **Extracted Fields**:
   * - Basic: id, object, filename, purpose, status
   * - Size: bytes (in bytes)
   * - Timestamps: created_at, expires_at (Unix timestamps)
   * - Status: status_details (error information if present)
   *
   * @param file - OpenAI FileObject
   * @returns Structured metadata view
   *
   * @example
   * ```typescript
   * const file = await service.retrieveFile('file-abc123xyz789');
   * const metadata = service.extractFileMetadata(file);
   * console.log(`File: ${metadata.filename}, Size: ${metadata.bytes} bytes, Status: ${metadata.status}`);
   * ```
   */
  extractFileMetadata(file: Files.FileObject): FileMetadata {
    return {
      id: file.id,
      object: file.object,
      bytes: file.bytes,
      created_at: file.created_at,
      filename: file.filename,
      purpose: file.purpose,
      status: file.status,
      status_details: file.status_details,
      expires_at: file.expires_at,
    };
  }

  /**
   * Sleep helper for polling
   * @param ms - Milliseconds to sleep
   */
  private sleep(ms: number): Promise<void> {
    return new Promise((resolve) => setTimeout(resolve, ms));
  }
}

/**
 * Structured view of OpenAI File metadata
 * Contains only fields from OpenAI response (no computed data)
 */
export interface FileMetadata {
  id: string;
  object: 'file';
  bytes: number;
  created_at: number;
  filename: string;
  purpose: string;
  status: 'uploaded' | 'processed' | 'error';
  status_details?: string | null;
  expires_at?: number | null;
}
